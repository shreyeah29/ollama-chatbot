# Ollama Chatbot ğŸ¤–

A locally hosted AI chatbot that integrates multiple open-source LLMs using [Ollama](https://ollama.ai/).  
Built with Flask and a lightweight frontend, the chatbot supports **multi-model conversations**, **real-time interaction**, and **privacy-preserving local execution**.

---

## âœ¨ Features
- ğŸ”€ **Multi-Model Support**: Integrates models like `llama3.2`, `phi`, `tinyllama`, and `neural-chat`.  
- ğŸ§  **Model Choice Prediction (MCP)**: Dynamically selects the most suitable model based on query type.  
- ğŸ’¬ **Real-Time Chat**: Instant responses with typing indicators and history persistence.  
- ğŸŒ **Web Interface**: Simple frontend with chat UI built using HTML, CSS, and JavaScript.  
- ğŸ”’ **Privacy-Preserving**: All models run locally through Ollama, ensuring data security.  

---

## ğŸ› ï¸ Tech Stack
- **Backend**: Python, Flask, Flask-CORS, requests, pytz  
- **Frontend**: HTML, CSS, JavaScript (static & templates)  
- **LLM Integration**: Ollama CLI (runs locally)  
- **Deployment**: Docker & Docker Compose (optional)  


